{"name":"论文","slug":"论文","count":1,"postlist":[{"title":"Attention Is All You Need","uid":"3166d93363a2489c2cf625cfd5e0d659","slug":"Attention-Is-All-You-Need","date":"2022-03-21T10:22:46.000Z","updated":"2022-03-21T10:36:25.215Z","comments":true,"path":"api/articles/Attention-Is-All-You-Need.json","keywords":null,"cover":[],"text":"原文链接：Attention Is All You Need Transformer模型结构Transformer模型遵循encoder-decoder架构，为编码器和解码器使用堆叠的自注意层和逐点全连接的层。 编码器与解码器的堆叠编码器由N&#x3D;6个相同的层堆叠组成，每层...","link":"","photos":[],"count_time":{"symbolsCount":"2.3k","symbolsTime":"2 mins."},"categories":[{"name":"论文","slug":"论文","count":1,"path":"api/categories/论文.json"}],"tags":[{"name":"Transformer","slug":"Transformer","count":1,"path":"api/tags/Transformer.json"}],"author":{"name":"JPFLY","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"Be Better!!","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}]}