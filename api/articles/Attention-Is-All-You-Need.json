{"title":"Attention Is All You Need","uid":"3166d93363a2489c2cf625cfd5e0d659","slug":"Attention-Is-All-You-Need","date":"2022-03-21T10:22:46.000Z","updated":"2022-03-21T10:36:25.215Z","comments":true,"path":"api/articles/Attention-Is-All-You-Need.json","keywords":null,"cover":[],"content":"<p>原文链接：<a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a></p>\n<h1 id=\"Transformer模型结构\"><a href=\"#Transformer模型结构\" class=\"headerlink\" title=\"Transformer模型结构\"></a>Transformer模型结构</h1><p>Transformer模型遵循encoder-decoder架构，为编码器和解码器使用堆叠的自注意层和逐点全连接的层。<br><img src=\"https://cdn.nlark.com/yuque/0/2022/png/22670636/1642824596420-9f7f3f11-e6e3-4569-8d41-bb1a97e5422b.png#clientId=uf05c64ed-c8c0-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=560&id=u5066a9bb&margin=%5Bobject%20Object%5D&name=image.png&originHeight=840&originWidth=617&originalType=binary&ratio=1&rotation=0&showTitle=true&size=106286&status=done&style=stroke&taskId=ub1aa36b7-51e6-42fd-be12-880e96cb9c4&title=Transformer%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84&width=411.3333333333333\" alt=\"image.png\" title=\"Transformer模型结构\"></p>\n<h2 id=\"编码器与解码器的堆叠\"><a href=\"#编码器与解码器的堆叠\" class=\"headerlink\" title=\"编码器与解码器的堆叠\"></a>编码器与解码器的堆叠</h2><p>编码器由N&#x3D;6个相同的层堆叠组成，每层有两个子层，一种是多头自注意力机制，另一种是简单的位置全连接前馈网络，层内与层间均使用了残差连接。即每个子层的输出可表示为$SublayerOutput&#x3D;LayerNorm(X+Sublayer(X))$，其中$Sublayer(X)$是本层实现的函数。</p>\n<p>解码器也由N&#x3D;6个相同的层堆叠组成，除了编码器中出现的两层，还引入了新的一层，对来自编码器的输出执行多头关注。每个子层周围仍使用残差连接，然后进行层规范化，作者还修改了解码器中的自注意子层，以防止当前位置对后续位置的影响。这种掩蔽，结合输出嵌入偏移一个位置，确保了位置i的预测只能依赖于位置小于i的已知输出。</p>\n<h2 id=\"注意力\"><a href=\"#注意力\" class=\"headerlink\" title=\"注意力\"></a>注意力</h2><p>注意力函数描述为将查询(query)和一组键值对(key-value)映射到输出，查询、键、值、输出都是向量。输出是值的加权计算，利用查询与键计算分配给每个值的权重。<br><img src=\"https://cdn.nlark.com/yuque/0/2022/png/22670636/1642828734078-af1a9f19-7ac1-426a-86cd-555dd5ee72d1.png#clientId=u0d085eb4-c87a-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=301&id=ue4c5e0f4&margin=%5Bobject%20Object%5D&name=image.png&originHeight=452&originWidth=856&originalType=binary&ratio=1&rotation=0&showTitle=true&size=67604&status=done&style=stroke&taskId=ufce8e5e0-511b-4a44-b90e-bcfb788aabd&title=%28%E5%B7%A6%29%E6%8C%89%E6%AF%94%E4%BE%8B%E7%BC%A9%E5%B0%8F%E7%9A%84%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%28%E5%8F%B3%29%E7%94%B1%E5%A4%9A%E4%B8%AA%E5%B9%B6%E8%A1%8C%E8%BF%90%E8%A1%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82%E6%9E%84%E6%88%90%E7%9A%84%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B&width=570.6666666666666\" alt=\"image.png\" title=\"&#40;左&#41;按比例缩小的点积注意力          &#40;右&#41;由多个并行运行注意力层构成的多头注意力\"></p>\n<h3 id=\"按比例缩小的点积注意力（Scaled-Dot-Product-Attention）\"><a href=\"#按比例缩小的点积注意力（Scaled-Dot-Product-Attention）\" class=\"headerlink\" title=\"按比例缩小的点积注意力（Scaled Dot-Product Attention）\"></a>按比例缩小的点积注意力（Scaled Dot-Product Attention）</h3><p>$Atention(Q,K,V)&#x3D;softmax(\\cfrac{QK^T}{\\sqrt{d_k}})V$</p>\n<p>$X$为原始单词输入</p>\n<p>$Q&#x3D;XW^Q$ $K&#x3D;XW^K$ $V&#x3D;XW^V$ $Q,K,V$本质上都是将输入$X$分别与一个可训练矩阵做乘积，从而提升模型的泛化能力</p>\n<p>$QK^T$：若$X$不与可训练矩阵相乘，则$QK^T&#x3D;XX^T$，向量1与向量2的转置做内积的大小意味着两个向量方向的相近程度，内积越大，两个向量方向越一致，以此来描述两个向量的相似程度。$Q$的第$i$行与$K^T$的第$j$列内积值越大，表示第$i$个查询更关注第$j$个值</p>\n<p>$\\sqrt{d_k}$：$d_k$为向量的纬度，假设向量query与key中元素都是相互独立且均值为0，方差为1的随机变量，则两向量内积后均值为0，方差为$d_k$。将$QK^T$除以$\\sqrt{d_k}$为了保持$QK^T$的均值为0，方差为1</p>\n<p>$softmax$将权重归一化到[0,1]之间</p>\n<h3 id=\"多头注意力（Multi-Head-Attention）\"><a href=\"#多头注意力（Multi-Head-Attention）\" class=\"headerlink\" title=\"多头注意力（Multi-Head Attention）\"></a>多头注意力（Multi-Head Attention）</h3><p>$MultiHead(Q,K,V)&#x3D;Concat(head_1,…,head_h)W^O\\where \\enspace head_i&#x3D;Attention(QW_i^Q,KW_i^K,VW_i^V)$</p>\n<p>通过$h$个不同的权重矩阵$QW_i^Q,KW_i^K,VW_i^V$来分别计算Scaled Dot-Product Attention，每组权重矩阵不同他们的关注点也有所不同，每个头计算出的结果拼接后与总的权重矩阵$W^O$相乘来决定每一个头的关注程度。</p>\n<p>对$Q,K,V$投影时，作者将每个token由$d_{model}$维投影到$d_{model}&#x2F;h$维，将每个头的计算结果拼接后经$W^O$投影回$d_{model}$维</p>\n<h3 id=\"注意力在Transformer中的应用\"><a href=\"#注意力在Transformer中的应用\" class=\"headerlink\" title=\"注意力在Transformer中的应用\"></a>注意力在Transformer中的应用</h3><p>在“编码解码注意力层”中，查询来自上一层的解码器，记忆的键和值来自编码器的输出，这使得解码器中的每个位置都能处理输入序列中的所有位置</p>\n<p>编码器包含自注意力层，自注意力层中，查询、键、值都是相同的，都来自编码器上一层的输出。编码器中的每个位置可以处理编码器先前层中的所有位置。</p>\n<p>解码器中的自注意力层允许解码器中的每个位置处理解码器中该位置前的所有位置。解码器自注意力层通过设置mask一是去除掉各种padding在训练过程中的影响，二是将输入进行遮盖，避免解码器看到后面要预测的东西，来保持回归特性（使$QK^T$中下标满足$[i][j], \\enspace i&lt;j$的值取0，即第i个单词不应注意到排在它后边的单词，同时padding部分也不应该去注意）</p>\n<h3 id=\"前馈网络（Position-wise-feed-forward-networks）\"><a href=\"#前馈网络（Position-wise-feed-forward-networks）\" class=\"headerlink\" title=\"前馈网络（Position-wise feed-forward networks）\"></a>前馈网络（Position-wise feed-forward networks）</h3><p>$FFN(x)&#x3D;max(0,xW_1+b_1)W_2+b_2$</p>\n<p>每一层经过注意力后，还要经过一个FNN，结构为线性-&gt;Relu-&gt;线性。目的是提供非线性空间变换，提高模型的表现能力。</p>\n<h3 id=\"位置编码（Positional-Encoding）\"><a href=\"#位置编码（Positional-Encoding）\" class=\"headerlink\" title=\"位置编码（Positional Encoding）\"></a>位置编码（Positional Encoding）</h3><p>$PE_{(pos,2i)}&#x3D;sin(pos&#x2F;1000^{2i&#x2F;d_{model}})$<br>$PE_{(pos,2i+1)}&#x3D;cos(pos&#x2F;1000^{2i&#x2F;d_{model}})$</p>\n<p>pos表示位置，i表示给向量的第几维编码，位置编码得到的向量与embedding得到的向量纬度是一致的。位置信息编码位于encoder和decoder的Embedding之后，每个block之前。Positional Encoding是transformer的特有机制，弥补了注意力机制无法利用序列位置信息的缺点。Positional Embedding的结果可以直接叠加于Embedding之上，使得每个符号的位置信息(Positional Encoding)和它的语义信息(Embedding)充分融合，并被传递到后续所有经过复杂变换的序列表达中去。</p>\n","text":"原文链接：Attention Is All You Need Transformer模型结构Transformer模型遵循encoder-decoder架构，为编码器和解码器使用堆叠的自注意层和逐点全连接的层。 编码器与解码器的堆叠编码器由N&#x3D;6个相同的层堆叠组成，每层...","link":"","photos":[],"count_time":{"symbolsCount":"2.3k","symbolsTime":"2 mins."},"categories":[{"name":"论文","slug":"论文","count":1,"path":"api/categories/论文.json"}],"tags":[{"name":"Transformer","slug":"Transformer","count":1,"path":"api/tags/Transformer.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Transformer%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84\"><span class=\"toc-text\">Transformer模型结构</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%BC%96%E7%A0%81%E5%99%A8%E4%B8%8E%E8%A7%A3%E7%A0%81%E5%99%A8%E7%9A%84%E5%A0%86%E5%8F%A0\"><span class=\"toc-text\">编码器与解码器的堆叠</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%B3%A8%E6%84%8F%E5%8A%9B\"><span class=\"toc-text\">注意力</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%8C%89%E6%AF%94%E4%BE%8B%E7%BC%A9%E5%B0%8F%E7%9A%84%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88Scaled-Dot-Product-Attention%EF%BC%89\"><span class=\"toc-text\">按比例缩小的点积注意力（Scaled Dot-Product Attention）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88Multi-Head-Attention%EF%BC%89\"><span class=\"toc-text\">多头注意力（Multi-Head Attention）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%9C%A8Transformer%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8\"><span class=\"toc-text\">注意力在Transformer中的应用</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C%EF%BC%88Position-wise-feed-forward-networks%EF%BC%89\"><span class=\"toc-text\">前馈网络（Position-wise feed-forward networks）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%88Positional-Encoding%EF%BC%89\"><span class=\"toc-text\">位置编码（Positional Encoding）</span></a></li></ol></li></ol></li></ol>","author":{"name":"JPFLY","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"Be Better!!","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{},"next_post":{}}