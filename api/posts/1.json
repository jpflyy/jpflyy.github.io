{"total":4,"pageSize":12,"pageCount":1,"data":[{"title":"DETR","uid":"cb267603e909abe660af40ea689ab423","slug":"DETR","date":"2022-03-21T13:29:46.000Z","updated":"2022-03-21T13:57:29.614Z","comments":true,"path":"api/articles/DETR.json","cover":[],"text":"原文链接：End-to-End Object Detection with Transformers 引言 目标检测的目标是为每个感兴趣的对象预测一组边界框和类别标签。当前大多数目标检测方法通过回归和分类来获得最终的检测结果，性能受到以下因素的显著影响：消除重复的预测后处理，锚点...","link":"","photos":[],"count_time":{"symbolsCount":"1.5k","symbolsTime":"1 mins."},"categories":[{"name":"论文","slug":"论文","count":4,"path":"api/categories/论文.json"}],"tags":[{"name":"Transformer","slug":"Transformer","count":4,"path":"api/tags/Transformer.json"},{"name":"cv","slug":"cv","count":3,"path":"api/tags/cv.json"}],"author":{"name":"JPFLY","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"Be Better!!","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},{"title":"Swin-Transformer","uid":"ec562ad6a6c391a3dd5ebfc55c638744","slug":"Swin-Transformer","date":"2022-03-21T13:27:37.000Z","updated":"2022-03-21T13:58:11.754Z","comments":true,"path":"api/articles/Swin-Transformer.json","cover":[],"text":"原文链接： Swin Transformer: Hierarchical Vision Transformer using Shifted Windows 引言 作者认为将语言领域的高性能转移到视觉领域的重大挑战可以用两种模式的差异来解释。 规模：与语言Transformer中作...","link":"","photos":[],"count_time":{"symbolsCount":"2.2k","symbolsTime":"2 mins."},"categories":[{"name":"论文","slug":"论文","count":4,"path":"api/categories/论文.json"}],"tags":[{"name":"Transformer","slug":"Transformer","count":4,"path":"api/tags/Transformer.json"},{"name":"cv","slug":"cv","count":3,"path":"api/tags/cv.json"}],"author":{"name":"JPFLY","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"Be Better!!","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},{"title":"Vision-Transformer","uid":"93157edbbe6cf715014badf14ef4775c","slug":"Vision-Transformer","date":"2022-03-21T13:12:21.000Z","updated":"2022-03-21T14:05:49.028Z","comments":true,"path":"api/articles/Vision-Transformer.json","cover":[],"text":"原文链接：AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE 引言 基于自注意力的架构Transformer已成为NLP领域的首选模型，它的主要方法是在大型文本语料库上进行预训练，然...","link":"","photos":[],"count_time":{"symbolsCount":"2.5k","symbolsTime":"2 mins."},"categories":[{"name":"论文","slug":"论文","count":4,"path":"api/categories/论文.json"}],"tags":[{"name":"Transformer","slug":"Transformer","count":4,"path":"api/tags/Transformer.json"},{"name":"cv","slug":"cv","count":3,"path":"api/tags/cv.json"}],"author":{"name":"JPFLY","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"Be Better!!","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},{"title":"Attention-Is-All-You-Need","uid":"7bb7fc64b6abbcc4ecc3eadbe6fac18b","slug":"Attention-Is-All-You-Need","date":"2022-03-21T12:37:39.000Z","updated":"2022-03-21T13:58:23.469Z","comments":true,"path":"api/articles/Attention-Is-All-You-Need.json","cover":[],"text":"原文链接：Attention Is All You Need Transformer模型结构 Transformer模型遵循encoder-decoder架构，为编码器和解码器使用堆叠的自注意层和逐点全连接的层。 编码器与解码器的堆叠 编码器由N=6个相同的层堆叠组成，每层有两个...","link":"","photos":[],"count_time":{"symbolsCount":"1.7k","symbolsTime":"2 mins."},"categories":[{"name":"论文","slug":"论文","count":4,"path":"api/categories/论文.json"}],"tags":[{"name":"Transformer","slug":"Transformer","count":4,"path":"api/tags/Transformer.json"}],"author":{"name":"JPFLY","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"Be Better!!","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}]}