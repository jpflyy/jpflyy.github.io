[{"id":"cb267603e909abe660af40ea689ab423","title":"DETR","content":"原文链接：End-to-End Object Detection with Transformers\r\n引言\r\n目标检测的目标是为每个感兴趣的对象预测一组边界框和类别标签。当前大多数目标检测方法通过回归和分类来获得最终的检测结果，性能受到以下因素的显著影响：消除重复的预测后处理，锚点的设计以及将目标框分配给锚点。作者将目标检测看做是一个直接的集合预测问题，简化了训练流程。DETR的主要特点是将二分匹配损失与解码器相结合。\r\n\r\n\r\nDETR一次性检测多个目标，通过损失函数唯一地匹配预测结果与标注信息。DETR整体结构只由ResNet和Transformer构成，不需要其他特殊的层\r\n\r\nDETR模型\r\n目标检测集预测损失（Object detection set prediction loss）\r\n\r\nDETR在一次通过解码器的过程中产生一个固定大小为N的预测集，N应明显大于图像中的目标数。作者用表示真实的目标集合，表示预测结果，将扩充为与同样的大小，∅表示无对象。为了找到两个集合元素的一一对应，寻找代价最小的N个元素的排列：\r\n对于标注，表示类别，表示边界框的四维向量（中心点，宽和高），定义类别预测为的概率为，预测的边界框为，定义如下： \r\n整体损失函数定义为类别预测的负对数与box loss的线性组合： \r\nbox loss定义为： \r\n其中为超参数，采用的是广义交并比（GIoU）来计算，普通的交并比IoU当两个框没有相交时永远为0，无法体现出距离信息，GIoU定义为：  其中C是包A，B包含在内的最小的边框，\r\n\r\nDETR架构\r\n\r\n\r\nDETR整体结构比较简单，骨干网络使用卷积神经网络提取图片特征，特征图的通道数相当于token的个数，特征图每个通道拉平成一维向量即是token向量，嵌入位置编码后输入进Transformer结构，Transformer输出经过简单的前馈神经网络做出最终的预测\r\n\r\n\r\n\r\nDETR Transformer的结构与原始Transformer类似。在Decoder中，Object queries表示N个预测目标，与原始Transformer的差别是DETR的query可以并行输入transformer进行查询，即解码器也使用了置换不变的先验知识，N个query是没有顺序之分的。此外，DETR中位置编码是在自注意力层内加入的，即编码器，解码器每一层都有位置编码嵌入，而原始Transformer仅在输入编码器或解码器时嵌入位置编码\r\nDERT Transformer的输出被前馈神经网络单独地解码成类标签和框坐标，最终产生N个预测\r\n通过使用自注意力，DETR可以通过成对的类标签与框坐标的关系进行全局推理，并且使用学习整个图像的上下文信息\r\n最终由一个具有ReLu激活函数，隐藏层维数为d的3层感知器，以及一个线性投影层来进行预测，FFN预测归一化的中心坐标，宽和高，线性投影层用softmax预测类标签\r\n\r\n消融实验\r\n编码器层数\r\n\r\n\r\n作者通过改变编码器层数来评估全局图像自注意力的重要性，没有编码器层整体AP将会下降\r\n\r\n\r\n\r\n作者可视化了经训练后模型编码器最后一层的输出：选取4个点附近区域观察其在整个图像上的关注度，从图中可以看出编码器的自注意力能够分离实例\r\n\r\n解码器层数\r\n\r\n\r\n从图中可以看出解码器对于性能也是十分重要的，训练过程中作者在解码器中使用了辅助损失，加入非极大值抑制仅在解码器第一层有明显的效果，自注意力抑制了重复的预测\r\n\r\n\r\n\r\n作者可视化了解码器层的注意力，可以看到解码层注意力关注目标的边界，从而提取出边框\r\n\r\n","slug":"DETR","date":"2022-03-21T13:29:46.000Z","categories_index":"论文","tags_index":"Transformer,cv","author_index":"JPFLY"},{"id":"ec562ad6a6c391a3dd5ebfc55c638744","title":"Swin-Transformer","content":"原文链接： Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\r\n引言\r\n作者认为将语言领域的高性能转移到视觉领域的重大挑战可以用两种模式的差异来解释。\r\n\r\n规模：与语言Transformer中作为基本处理元素的单词符号不同，视觉元素在规模上有很大的变化，例如目标检测，不同的目标规模差别是很大的，在ViT中，token的大小是固定的，每一层处理的都是patch size大小的区域，虽然通过注意力来得到一个全局的理解，但是对多尺度特征的理解相对较弱\r\n分辨率：图像分辨率比段落中文字要高很多，一些视觉任务如语义分割需要在像素级进行密集预测，而Transformer在高分辨率图像上是难以处理的，它的计算复杂度是图像尺寸的平方\r\n\r\n\r\n\r\nSwin Transformer通过合并patch来构建分层特征映射，只在局部窗口内计算自注意力，因此对于图像大小具有线性复杂度，而ViT中，对于图像大小计算复杂度为二次\r\n\r\n\r\n\r\n在连续的自注意力层之间移动窗口，在第l层，窗口为规则划分，在第l+1层，窗口向右下移动，得到的新的窗口跨越了上一层中的不同窗口，从而将不同的区域联系在一起\r\n\r\n方法\r\n总体架构\r\n\r\n\r\n输入H×W×3的图片，首先分为4×4大小的patch，得到H/4×W/4个48维向量，对这些向量进行线性映射得到Transformer层的输入，H/4×W/4个token，每个token纬度为C\r\nSwin Transformer Block输入输出尺寸一致，内部自注意力的计算是基于窗口的\r\nPatch Merging层将4个patch合并为1个，降低token个数，将4个patch合并为1个导致向量纬度变为4C，然后在C的纬度上通过1×1的卷积将纬度变为2C。相当于对输入的张量，宽和高减半，通道数变为原来的2倍\r\nW-MSA和SW-MSA分别是具有规则窗口和移动窗口的多头自注意力，窗口大小为M×M\r\n\r\n基于移动窗口的自注意力\r\nW-MSA基于规则窗口的多头自注意力，有效减少了计算复杂度，但是由于注意力是在窗口内部计算的，无法得到窗口之间的关注度，因此每个W-MSA的Transformer后都跟一个SW-MSA的Transformer，基于移动窗口的自注意力，向右下方移动窗口后，新的窗口包含了之前不同窗口内的信息，这样在窗口内计算自注意力就能得到窗口间的关注度。但是移动窗口后，有的窗口不能保证是固定的大小，窗口数目增多了，不利于在一个batch中计算，即使使用padding，计算复杂度也提高了。\r\n\r\n\r\n作者提出了一种高效的方法来解决移动窗口后产生的问题，首先将不完整的窗口循环移位拼接在右下方，补全不完整的窗口，最终得到的窗口数目与规则窗口数目保持一致。循环移位后本不该产生注意力的块由于拼接在一个窗口，运算可能会产生注意力，为了消除这种影响，作者定义了一些掩码，让拼接出的窗口中不同的部分各自计算自注意力。最后，需要将拼接还原，恢复原有的语义信息\r\n\r\n\r\n\r\n每个patch是一个向量，自注意力计算时，需要计算，假设窗口大小为7×7，则一个窗口内有49个patch，的大小为49×C，自注意力中就是。若第i行向量不需要与第j行向量计算注意力，那么只需在注意力计算后的结果矩阵中的第i行第j列位置和第j行第i列位置加上一个很大的负值（例如-100）即可，这个负值经softmax可得到接近于0的结果。\r\n\r\n模型变体\r\n\r\n\r\n\r\nSwin-T\r\nC=96\r\nlayer numbers = {2, 2, 6, 2}\r\n\r\n\r\n\r\n\r\nSwin-S\r\nC=96\r\nlayer numbers = {2, 2, 18, 2}\r\n\r\n\r\nSwin-B\r\nC=128\r\nlayer numbers = {2, 2, 18, 2}\r\n\r\n\r\nSwin-L\r\nC=192\r\nlayer numbers = {2, 2, 18, 2}\r\n\r\n\r\n\r\n实验\r\n分类任务\r\n\r\n\r\nImageNet-1K包含1.28M张训练图片和50K张测试图片，来自1000个类别，（a）是直接在ImageNet-1K上训练得到的结果，在较小的数据集上，ViT表现稍差，Swin Transformer与EffNet-B7不分伯仲，EffNet-B7优势在于输入图片尺寸更大，参数量较小\r\nImageNet-1K包含14.2M张图片和22K类，ViT和Swin Transformer经大训练集预训练后优势显著，有着更高的准确率和更少的参数\r\n\r\n目标检测任务\r\n\r\n\r\n（a）：骨干网络ResNet-20与Swin-T在不同方法上比较，Swin-T更优\r\n（b）：不同的骨干网络，在3类参数规模下的效果对比，Swit-T更优\r\n（c）：任意网络任意方法任意规模，最优结果是Swin-L产生的\r\n\r\n语义分割任务\r\n\r\n\r\n在计算成本相似的条件下，Swin仍是最好的，使用Swin-L在ImageNet-22K预训练后，mIoU比之前最佳模型SETR高了3.2，而SETR参数量比Swin-L高\r\n\r\n","slug":"Swin-Transformer","date":"2022-03-21T13:27:37.000Z","categories_index":"论文","tags_index":"Transformer,cv","author_index":"JPFLY"},{"id":"93157edbbe6cf715014badf14ef4775c","title":"Vision-Transformer","content":"原文链接：AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\r\n引言\r\n\r\n基于自注意力的架构Transformer已成为NLP领域的首选模型，它的主要方法是在大型文本语料库上进行预训练，然后在较小的特定于任务的数据集上进行微调\r\n在计算机视觉中，受NLP启发，作者将Transformer直接应用于图片，做尽可能小的修改。作者将图片分为小的patch（16×16），每个patch通过全连接做线性嵌入得到序列，相当于一个patch是一个单词，一个图片是一个句子。训练方式为有监督方式\r\n在中等规模数据集如ImageNet上训练时，没有强正则化，模型精度比同等规模的ResNet低几个百分点，作者认为这是可以预见的，因为Transformer缺少一些卷积神经网络固有的归纳偏置，例如平移等变性（ translation equivariance）和局部性（locality）。局部性是指假设图片相邻区域具有相邻的特征，平移等变性是指无论先做平移还是先做卷积，最后的结果是不变的\r\n作者在更大规模的数据集上训练，大规模的数据集战胜的归纳偏置，ViT在更大规模数据集（14M-300M）上的结果接近或击败了最先进的水平\r\n\r\n相关工作\r\n\r\n将Transformer用于图像的最简单方法是直接将输入二维图片拉平为一维作为Transformer输入，这会产生巨大的计算量\r\n过去采用的一些方法：\r\n\r\n使用卷积网络产生的特征图作为Transformer的输入\r\n对稀疏的点做注意力\r\n将图片分成小的窗口，每个窗口作为Transformer的输入\r\n将2d图像矩阵分别在高度和宽度方向上使用1d向量作为Transformer的输入\r\n\r\n\r\n模型\r\nVision Transformer\r\n\r\nPatch Embedding\r\n输入图像224×224×3，将每个图像分为小的patch，patchsize=16×16，那么原图则可以分成(224/16)×(224/16)=14×14=196个patch，每个patch大小为16×16×3\r\n线性映射层(Linear Projection)用于拉平patch，实质上是一个全连接层，全连接层纬度为768×D，其中768是patch的纬度(16×16×3)，D是Transformer中所有层的输出向量纬度，把一个16×16×3的patch图像拉平为一个D维向量。最终相当于输入的具有196个token，每个token向量的纬度为768。线性映射层纬度为768×D，为196×D的矩阵\r\nPosition Embedding\r\n给每个token进行位置编码，编码结果直接叠加在上，进行整体学习，作者使用的是1D位置编码，也尝试了2D编码，但是整体效果差别不大，可能的原因是patch不是一个个像素，学习位置信息相对容易。作者也尝试了不加位置编码，效果要比加上位置编码差一些。\r\nClass Token\r\n一个额外的token，用于表示类别，纬度是D，最终输入到Transformer中是197×D。最终分类时，将输出矩阵中class token对应的向量作为图像整体特征向量用于分类。作者使用class token目的是为了与transformer保持一致，不加class token最终采取平均池化作为图像特征也是可行的。\r\n流程公式表示\r\n\r\n$ z_0=[x_{class};x_p1E;x_p2E;...;x_p^NE]+E_{pos}  E∈R{(P2C)×D}，E_{pos}∈R^{(N+1)×D} $\r\n       \r\n       \r\n\r\n\r\n\r\n首先将原始图像分为N个patch，每个patch进行线性映射，生成一个D维向量，N个D维向量与一个class token向量拼接为一个(N+1)×D的矩阵，在叠加上位置编码得到Transformer的输入\r\nLN是layer norm层归一化，在MSA（多头自注意力）之前需要进行层归一化，并且需要残差连接，叠加上一个MLP（多层感知机）的输出，得到最终输出，L表示Transformer有几层\r\nMLP处理过程与MSA类似，都需要先进行层归一化，MLP需要叠加上一个MSA的输入，最终输出\r\n最终提取class token对应的向量作为图像特征进行分类任务\r\n\r\n实验\r\n模型超参数\r\n作者共定义了三种模型，并进行了训练。Layers表示Transformer的层数，Hidden size D表示Transformer中统一的token向量纬度，MLP size一般设置为4D大小，Heads是多头自注意力中的头数目。\r\n\r\n\r\nViT-H/14表示用ViT-Huge模型，patch size取14×14。\r\n\r\n实验结果\r\n\r\n\r\n采用ViT可以在主流的图像分类基准上取得很好的效果，尤其是ViT-H/14，在各个数据集上全部超越了ResNet125×4，虽然领先幅度不是特别明显，但是显著的是所需要的计算资源大大减少。\r\n\r\n\r\n\r\n从上图可以看出在数据集相对小的时候，ResNet能取得更好的效果，数据集达到300M时，ViT取得更好的效果。原因是ViT没有ResNet那样使用强正则化以及卷积网络固有的归纳偏置\r\n\r\n\r\n\r\n上图展示了Transformer不同层各个头的平均注意距离，单位是像素。可以看到在Transformer的第0层，就已经关注到了较全局的信息，而卷积神经网络首层只能关注非常局部的信息\r\n\r\n自监督\r\n因为Transformer在NLP中训练采用自监督的方式，作者也使用自监督对图像分类做了实验，作者采用masked patch prediction来进行初步实验，破坏50%的patch，相比于有监督的训练，使用ViT-B/16在InageNet上能达到79.9%的准确率，比有监督低了4%，无监督仍是有效的。\r\n","slug":"Vision-Transformer","date":"2022-03-21T13:12:21.000Z","categories_index":"论文","tags_index":"Transformer,cv","author_index":"JPFLY"},{"id":"7bb7fc64b6abbcc4ecc3eadbe6fac18b","title":"Attention-Is-All-You-Need","content":"原文链接：Attention Is All You Need\r\nTransformer模型结构\r\nTransformer模型遵循encoder-decoder架构，为编码器和解码器使用堆叠的自注意层和逐点全连接的层。 \r\n编码器与解码器的堆叠\r\n编码器由N=6个相同的层堆叠组成，每层有两个子层，一种是多头自注意力机制，另一种是简单的位置全连接前馈网络，层内与层间均使用了残差连接。即每个子层的输出可表示为，其中是本层实现的函数。\r\n解码器也由N=6个相同的层堆叠组成，除了编码器中出现的两层，还引入了新的一层，对来自编码器的输出执行多头关注。每个子层周围仍使用残差连接，然后进行层规范化，作者还修改了解码器中的自注意子层，以防止当前位置对后续位置的影响。这种掩蔽，结合输出嵌入偏移一个位置，确保了位置i的预测只能依赖于位置小于i的已知输出。\r\n注意力\r\n注意力函数描述为将查询(query)和一组键值对(key-value)映射到输出，查询、键、值、输出都是向量。输出是值的加权计算，利用查询与键计算分配给每个值的权重。 \r\n按比例缩小的点积注意力（Scaled Dot-Product Attention）\r\n\r\n为原始单词输入\r\n   本质上都是将输入分别与一个可训练矩阵做乘积，从而提升模型的泛化能力\r\n：若不与可训练矩阵相乘，则，向量1与向量2的转置做内积的大小意味着两个向量方向的相近程度，内积越大，两个向量方向越一致，以此来描述两个向量的相似程度。的第行与的第列内积值越大，表示第个查询更关注第个值\r\n：为向量的纬度，假设向量query与key中元素都是相互独立且均值为0，方差为1的随机变量，则两向量内积后均值为0，方差为。将除以为了保持的均值为0，方差为1\r\n将权重归一化到[0,1]之间\r\n多头注意力（Multi-Head Attention）\r\n\r\n通过个不同的权重矩阵来分别计算Scaled Dot-Product Attention，每组权重矩阵不同他们的关注点也有所不同，每个头计算出的结果拼接后与总的权重矩阵相乘来决定每一个头的关注程度。\r\n对投影时，作者将每个token由维投影到维，将每个头的计算结果拼接后经投影回维\r\n注意力在Transformer中的应用\r\n在“编码解码注意力层”中，查询来自上一层的解码器，记忆的键和值来自编码器的输出，这使得解码器中的每个位置都能处理输入序列中的所有位置\r\n编码器包含自注意力层，自注意力层中，查询、键、值都是相同的，都来自编码器上一层的输出。编码器中的每个位置可以处理编码器先前层中的所有位置。\r\n解码器中的自注意力层允许解码器中的每个位置处理解码器中该位置前的所有位置。解码器自注意力层通过设置mask一是去除掉各种padding在训练过程中的影响，二是将输入进行遮盖，避免解码器看到后面要预测的东西，来保持回归特性（使中下标满足的值取0，即第i个单词不应注意到排在它后边的单词，同时padding部分也不应该去注意）\r\n前馈网络（Position-wise feed-forward networks）\r\n\r\n每一层经过注意力后，还要经过一个FNN，结构为线性-&gt;Relu-&gt;线性。目的是提供非线性空间变换，提高模型的表现能力。\r\n位置编码（Positional Encoding）\r\n \r\npos表示位置，i表示给向量的第几维编码，位置编码得到的向量与embedding得到的向量纬度是一致的。位置信息编码位于encoder和decoder的Embedding之后，每个block之前。Positional Encoding是transformer的特有机制，弥补了注意力机制无法利用序列位置信息的缺点。Positional Embedding的结果可以直接叠加于Embedding之上，使得每个符号的位置信息(Positional Encoding)和它的语义信息(Embedding)充分融合，并被传递到后续所有经过复杂变换的序列表达中去。\r\n","slug":"Attention-Is-All-You-Need","date":"2022-03-21T12:37:39.000Z","categories_index":"论文","tags_index":"Transformer","author_index":"JPFLY"}]